<!------------------------------------------------------------------------------
- "THE BEER-WARE LICENSE" (Revision 42):
- <francois.ripault@epita.fr> wrote this file. As long as you retain this
- notice you can do whatever you want with this stuff. If we meet some day, and
- you think this stuff is worth it, you can buy me a beer in return.
- Francois Ripault
----------------------------------------------------------------------------->

Redesign the library from scratch:

Benchmarks definition:
======================
  Benchmarks are created using the `defbench` function, whose prototype is
  the following:
    `(defbench name number function argument-list-list)`
  `function` will be called `number` times with argument-list contained in
  `argument-list-list`. If categories are used in the argument-list of a
  defbench, a benchmark will be added for each value contained in the category.
  If multiple categories are used, a benchmark will be added for each element
  generated by the cartesian product of the categories.

Category definition:
====================
  Categories are defined with `defcat`, its prototype being:
    `(defcat name arg-list)`
  To make reference to a category when declaring benchmarks, the syntax is
  the following: `cat\name` where `name` is the name of the category.

Execution of benchmarks:
========================
  The function `run-bench` will executed the defined benchmarks. Its prototype
  is the following:
    `(run-bench &optional bench-list)`
  If no `bench-list` is provided, all the benchmarks will be run. Else, only
  the specified benchmarks will be executed.
  The `run-bench` functions returns an `output` object, which can be
  serialized or used with the function `output`.

Output of Benchmarks:
=====================
  `output` pretty prints the results on the standard output. It takes a
  selector for the output format: either a symbol, which represents the
  output formats given by fr-clpl, or an object, which will define the output
  format.
  The objective is to provide to the user various ways of output:
    - sbcl: this is the default output format by sbcl
    - latex: this is a latex tabular format
    - graphic: outputs a graph of the benchmarks.
  In addition, this function should provide a way of selecting the results
  from the `output` object, in order to offer a greater control on the
  results (i.e, show only performances of an algorithm with different
  arguments, or show the performance of many algorithms, etc ...)

Metaprofiling:
==============
  Fr-clpl should also provide a way of applying benchmarks on various compilers
  and/or version of a same package. The filer `loader.lisp` should allow the
  management of such benchmarks.
